---
title: 'P&S-2022: Lab assignment 2'
author: "Bohdana Zubrytska(task 3, task 4)",
Name2, Name3"
output:
  html_document:
    df_print: paged
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
# your team id number 
                          ###
id <- 34                  ### Change to the correct id!
                          ###
set.seed(id)
p <- id/100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
cat("The matrix G is: \n") 
G  
cat("The matrix H is: \n") 
H
cat("The product GH must be zero: \n")
(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# generate N messages
N <- 100
message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(100)

# calculate code for messages
codewords <- (messages %*% G) %% 2

cat("messages\n")
head(messages)
cat("codewords\n")
head(codewords)
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
# generate an N*7 matrix consisting of independent Bernoulli r.v. with parameter p
errors <- matrix(
  sample(c(0,1), N*7, replace = TRUE, prob = c(1-p, p)),
  nrow = N, byrow = TRUE
)

# impose an error matrix on messages 
received <- (codewords + errors) %% 2
cat("received\n")
head(received)
```

The next steps include detecting the errors in the received messages, correcting them, and then decoding the obtained messages. After this, you can continue with calculating all the quantities of interest

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

```{r}
# calculatе orror position ((1..7) or 0 - no error)
Z <- (received %*% H) %% 2
idx <- Z %*% c(1,2,4) 

corrected <- received

# correct each message
for (i in seq_len(nrow(corrected))) {
  j <- idx[i]          # error position (1..7) or 0 - no error
  if (j > 0) {
    corrected[i, j] <- 1 - corrected[i, j]
  }
}

cat("corrected\n")
head(corrected)
```

p_hat - the sample estimate of the probability of correct decoding.

```{r}
decoded <- corrected[, c(3,5,6,7), drop=FALSE]

success <- apply(messages == decoded, 1, all)
p_hat   <- mean(success) 
p_hat
```

p_theory - the theoretical probability that a single transmitted message is decoded correctly.

Let K be the number of bit errors in a 7-bit codeword during transmission. Assuming each bit flips independently with probability p, we have K∼Binomial(7,p). The Hamming(7,4) decoder succeeds if K≤1. Therefore, the theoretical success probability is $\mathbf{p^*} = Pr(K \le 1) = \sum\binom{7}{k} p^{k} (1-p)^{7-k} = Pr(K = 0) + Pr(K = 1) = (1-p)^7 + 7p(1-p)^6$

```{r}
p_theory <- (1-p)^7 + 7*p*(1-p)^6 # p_star
p_theory
```

1.  $\hat p$ expected to be close to $p^*$ when N is large because each trial’s success indicator is i.i.d. Bernoulli($p^∗$), so the sample mean $\hat p$ converges to its expectation $p^*$ by the Law of Large Numbers.

2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
    $$
     \operatorname{Var}(\hat p)
     = \frac{1}{N^{2}}\sum_{i=1}^{N}\operatorname{Var}(X_i)
     = \frac{p^*\!\left(1-p^*\right)}{N}
     $$ $$
     \operatorname{\hat{Var}}(\hat p)
     = \frac{1}{N^{2}}\sum_{i=1}^{N}\operatorname{Var}(X_i)
     = \frac{\hat p\!\left(1-\hat p\right)}{N}
     $$\
    $$
     \operatorname{\hat{\sigma}}(\hat p)
     = \sqrt{\frac{\hat p\!\left(1-\hat p\right)}{N}}
     $$\
    Using the 95% (≈ two-sigma) rule,\
    $$
    \Pr\!\left(\,|\hat p - p^*| \le 2\hat{\sigma}\right) \approx 0.95
    $$

```{r}
e = 2 * sqrt(p_hat*(1-p_hat)/N)
e
```

3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
    Using the 95% (≈ two-sigma) rule $$
    \varepsilon = 2\sigma
    $$ $$
    \hat{\varepsilon} = 2\sqrt{\frac{\hat p\!\left(1-\hat p\right)}{N}} \le 0.03
    $$ $$
    N \ge \frac{\hat p\!\left(1-\hat p\right)}{(\frac{0.03}{2})^2}
    $$

```{r}
new_N = ceiling(p_hat*(1-p_hat)/(0.015*0.015))
new_N
```

4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

```{r}
message <- codewords[, c(3,5,6,7), drop = FALSE]
received_message <- received[, c(3,5,6,7), drop = FALSE]

k <- rowSums(received_message != message)
sum_for_k <- table(factor(k, levels = 0:4))
emp <- as.numeric(sum_for_k) / N


theory <- dbinom(0:4, size = 4, prob = p)
```

```{r}
bp <- barplot(
  emp,
  names.arg = 0:4,
  main = "Errors in 4-bit message",
  xlab = "Number of errors (k)",
  ylab = "Relative frequency",
  ylim = c(0, max(emp) * 1.15)
)

# add small percentage labels above bars
text(x = bp, y = emp, labels = paste0(round(100*emp, 1), "%"), pos = 3, cex = 0.9)

lines(bp, theory, lwd = 2)

legend("topright",
       legend = sprintf("Theoretical Binomial(4, p=%.2f)", p),
       lty = 1, lwd = 2, bty = "n")
```

How we can see empirical histogram closely matches the theoretical Binomial(4, p=0.34) curve.

### Task 2.

In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

```{r}
T <- 30.1  #years
T <- T * 365 * 24 * 60 * 60  #seconds
lambda <- log(2) / T  

N_a = 6 * 10^23
M <- 136.907089
m <- id * 10^(-6)
N <- m * N_a / M

mu <- N * lambda
print(mu)
K <- 1e3

```

#### Next, calculate the parameters of the standard normal approximation

For independent and identically distributed Poisson random variables\
$$
X_1, X_2, \dots, X_n \sim \text{Poisson}(\mu),
$$ we have\
$$
E[X_i] = \mu, \quad \text{and} \quad \mathrm{Var}(X_i) = \mu.
$$ The sample mean is\
$$
\overline{X} = \frac{1}{n}\sum_{i=1}^{n} X_i.
$$ Then its expected value and variance are\
$$
E[\overline{X}] = \mu, \quad \mathrm{Var}(\overline{X}) = \frac{\mu}{n}.
$$ By the **Central Limit Theorem (CLT)**, for large $n$, $$
\frac{\overline{X} - \mu}{\sqrt{\mu / n}} \sim \mathcal{N}(0,1),
$$ so the standard deviation (sigma) of the sample mean is\
$$
\sigma = \sqrt{\frac{\mu}{n}}.
$$

```{r}
mu <- N * lambda
sigma <- sqrt(mu/n)
```

#### We can now plot ecdf and cdf

```{r}
n_values <- c(5, 10, 50, 10000)
max_diff_by_n <- numeric(length(n_values))
names(max_diff_by_n) <- n_values

for (k in seq_along(n_values)){
  n <- n_values[k]
  sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
  sigma <- sqrt(mu/n)
  xlims <- c(mu-3*sigma,mu+3*sigma)
  Fs <- ecdf(sample_means)
  plot(Fs, 
       xlim = xlims, 
       ylim = c(0,1),
       col = "blue",
       lwd = 2,
       main = "Comparison of ecdf and cdf")
  curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
  
  xs <- seq(min(sample_means), max(sample_means), length.out = K)
  F_emp <- ecdf(sample_means)(xs)
  F_norm <- pnorm(xs, mean = mu, sd = sigma)
  
  max_diff_by_n[k] <- max(abs(F_emp - F_norm))

}
max_diff_by_n
```

The comparison between the empirical and theoretical cumulative distribution functions shows that the difference between them decreases as the sample size n grows. However, for the suggested values n = 5, 10, 50, the difference is quite small already, so the improvement is not clearly visible. To better demonstrate the trend, we also tested a much larger sample size (n = 10 000), where the gap between the two curves becomes noticeably smaller. This confirms that as n increases, the distribution of sample means becomes closer to normal, as expected.

Next**, proceed with all the remaining steps**

```{r}
T <- 30.1  #years
T <- T * 365 * 24 * 60 * 60  #seconds
lambda <- log(2) / T  

N_a = 6 * 10^23
M <- 136.907089
m <- id * 10^(-6)
N <- m * N_a / M

mu <- N * lambda

a <- 8e8
p <- 0.95
```

We derive formula for the probability condition $$
P(S_n \le 8 \times 10^8) \ge 0.95
$$ by finding $$
P(S_n > 8 \times 10^8) \le 0.05
$$ using Markov's inequality, which states that $$
P(S_n > 8 \times 10^8) \le \frac{E(S_n)}{8 \times 10^8}
$$ the following inequality for $n$: $$
\frac{n \cdot \mu}{8 \times 10^8} \le 0.05
$$

```{r}
n_markov <- floor((1-p) * a / mu)
cat("n_markov =", n_markov, "\n")
```

We need $$
P(S_n \le a) \ge 0.95
$$ Similarly, we transform it to $$
P(S_n > a) \le 0.05
$$ Let's use **Chernoff inequality**: $$
P(S_n > a) \le e^{-s \cdot a} \cdot M(s)
$$ For Poisson distributed random variable $$
M(s) = e^{n \mu (e^{s} - 1)}
$$ So we have such inequality: $$
P(S_n \ge a) \le e^{-s \cdot a + n \mu (e^{s} - 1)}
$$ To find maximum $n$ we need to solve $$
e^{-s \cdot a + n \mu (e^{s} - 1)} = 0.05
$$ which gives $$
-s \cdot a + n \mu (e^{s} - 1) = \ln(0.05)
$$ whence $$
e^{s} = \frac{\ln(0.05)}{\mu n} + \frac{a s}{\mu n} + 1
$$ We can neglect the very small value of $\frac{\ln(0.05)}{\mu n}$ because of $\mu$ in the denominator—it is very close to zero. Now let's estimate $$
e^{s} = \frac{a s}{\mu n} + 1
$$ Since according to Chernoff inequality $s > 0$ and we want to find maximum of $e^{s}$,\
we use a tangent line of $e^{s}$ at point $s = 0$: $$
f(s) = s - 1
$$ Looking at the coefficient near $s$, we want it to be greater than 1 so we can get a positive root of the equation to satisfy the condition.\
Thus our coefficient $$
\frac{a}{\mu n} > 1
$$ from where $$
n < \frac{a}{\mu}
$$ Clearly, $$
n_{\text{max}} = \left\lfloor \frac{a}{\mu} \right\rfloor
$$

```{r}
n_chernoff <- floor(a / mu)
cat("n_chernoff =", n_chernoff, "\n")
```

We need $$
P(S_n \le a) \ge 0.95
$$ By **Central Limit Theorem (CLT)** we can approximate $$
\frac{S_n - n \mu}{\sqrt{n \mu}} \sim N(0, 1)
$$ So we can use the quantile for 0.95: $$
qnorm(0.95) = 1.644854
$$ which gives $$
\frac{a - n \mu}{\sqrt{n \mu}} = 1.644854
$$ Now we just solve this for $n$.

```{r}
sigma = sqrt(mu)
n_clt <- floor(((-qnorm(0.95) * sigma + sqrt((qnorm(0.95) * sigma)^2 + 4 * mu * a)) / (2 * mu))^2)
cat("n_clt =", n_clt, "\n")
```

```{r}
simulate_prob <- function(n, mu, K, a) {
  sims <- replicate(K, sum(rpois(n, mu)))
  mean(sims <= a)
}

p_markov  <- simulate_prob(n_markov, mu, K, a)
p_chernoff <- simulate_prob(n_chernoff, mu, K, a)
p_clt     <- simulate_prob(n_clt, mu, K, a)

cat("\nEmpirical probabilities P(S_n ≤ 8×10^8):\n")
cat(" Markov bound   (n =", n_markov,  "):", round(p_markov, 4), "\n")
cat(" Chernoff bound (n =", n_chernoff, "):", round(p_chernoff, 4), "\n")
cat(" CLT bound      (n =", n_clt,      "):", round(p_clt, 4), "\n")

```

All three theoretical approaches (Markov, Chernoff, and CLT) give extremely small upper bounds for n. The Markov inequality even results in n = 0, which is very uninformative. For n = 7 (Chernoff and CLT), the empirical probability P(S_n \< 8 \* 10\^8) = 1, which far exceeds the target level of 0.95.

This happens because the Poisson parameter is very large, mu -\> 1.09 \* 10\^8; even a single sample already yields an expected number of decays over the critical threshold 8 \* 10\^8. As a result, the theoretical bounds become overly conservative: they satisfy the probability condition but at unrealistically small n.

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

#### First, generate samples an sample means:

```{r}
nu1 <- 34 + 10
K <- 1000
n <- 5
sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- 1/nu1 # E[X]
sigma <- 1/(nu1*sqrt(n))# sqrt(Var[X]/n)


```

#### We can now plot ecdf and cdf

```{r}
analyze_sample_means <- function(n, nu1, K) {
  sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))
  mu <- 1/nu1
  sigma_squared <- (1/nu1^2) / n
  sigma <- sqrt(sigma_squared)
  
  #empirical CDF
  sorted_means <- sort(sample_means)
  ecdf_values <- (1:K) / K
  
  #theoretical normal CDF at the same points
  normal_cdf <- pnorm(sorted_means, mean = mu, sd = sigma)
  
  max_diff <- max(abs(ecdf_values - normal_cdf))
  
  plot(sorted_means, ecdf_values, type = "s", col = "blue", lwd = 2,
       main = paste("ecdf and normal cdf for n =", n),
       xlab = "Sample mean", ylab = "Cumulative probability",
       ylim = c(0, 1))
  lines(sorted_means, normal_cdf, col = "red", lwd = 2)
  legend("bottomright", 
         legend = c("Empirical CDF", 
                    paste("Normal(", round(mu, 4), ",", round(sigma_squared, 6), ")")),
         col = c("blue", "red"), lwd = 2)
  grid()
  

  cat("\n  Results for n =", n, "\n")
  cat("Theoretical mean (mu):", mu, "\n")
  cat("Theoretical variance (sigma^2):", sigma_squared, "\n")
  cat("Maximum difference between CDFs:", max_diff, "\n")
  
  return(list(mu = mu, sigma_squared = sigma_squared, max_diff = max_diff))
}

#analyze for n = 5, 10, 50
par(mfrow = c(1, 3))

results_n5 <- analyze_sample_means(5, nu1, K)
results_n10 <- analyze_sample_means(10, nu1, K)
results_n50 <- analyze_sample_means(50, nu1, K)

par(mfrow = c(1, 1))


cat("As n increases, the maximum difference decreases:\n")
cat("n=5:  max_diff =", round(results_n5$max_diff, 4), "\n")
cat("n=10: max_diff =", round(results_n10$max_diff, 4), "\n")
cat("n=50: max_diff =", round(results_n50$max_diff, 4), "\n")
cat("This confirms the Central Limit Theorem: larger sample sizes\n")
cat("lead to better normal approximation.\n")


```

2.  The place is safe if: Number of clicks in 1 minute \<= 100. This is equivalent to: Time until 100th click \> 60 seconds (1 minute). Event of interest: P(S \> 60) \>= 0.95, where S = X1 + ... + X100. We need to find the maximum number of radioactive samples ($N$) that keeps the probability of the location being safe ($\mathbb{P}(S > 60)$) at $\mathbf{0.95}$ or higher.

<!-- -->

1.  Firstly, check with Markov Inequality

    P(S \> 60) \>= 1 - E[S]/60 For P(S \> 60) \>= 0.95, we try to solve 1 - E[S]/60 \<= 0.95 E[S] = 100/nu. 100/(60\*nu) \>= 0.05 =\> nu \<= 100/(60\*0.05) = 33.333

```{r}
cat("\n Method 1: Markov Inequality\n")

N_markov <- ceiling(33.33 / nu1)
cat("Markov bound: N >=", 33.33/nu1, "=> N >=", N_markov, "\n")
```

2.  Chernoff Bound P(S \< 60) \<= 0.05 The optimized Chernoff bound for P(S \< (1-delta)E[S]). E[S] = 100/nu. We want 60 = (1-delta) \* 100/nu, so delta = 1 - 60\*nu/100

```{r}
cat("\n  Method 2: Chernoff Bound \n")
nu_chernoff <- 100/60 * (1 + sqrt(-log(0.05)/100))
N_chernoff <- ceiling(nu_chernoff / nu1)
cat("Chernoff bound: nu >=", round(nu_chernoff, 2), "=> N >=", N_chernoff, "\n")
```

3.  Central Limit Theorem S \~ N(100/nu, 100/nu\^2). We want P(S \<= 60) = 0.05 Standardizing: (60 - 100/nu) / (10/nu) = qnorm(0.05)

```{r}
cat("\n   Method 3: Central Limit Theorem \n")
# 60*nu - 100 = -16.45
# nu = (100 - 16.45)/60 ≈ 1.393
nu_clt <- (100 - 10*qnorm(0.05)) / 60
N_clt <- ceiling(nu_clt / nu1)
cat("CLT approximation: nu >=", round(nu_clt, 4), "=> N >=", N_clt, "\n")
```

```{r}
nu1 <- 44
N_clt <- 1 
qnorm_05 <- qnorm(0.05)
#theoretical minimum nu required by CLT to achieve P(S <= 60) = 0.05
nu_clt_min <- (100 - 10*qnorm_05) / 60

cat("Markov gives weakest bound. Chernoff gives tighter bound. CLT gives the tightest and most accurate bound\n")
cat("We will use CLT result: N =", N_clt, "\n")

cat("\n Simulation Verification \n")
nu_pred <- nu_clt_min 
N_check <- nu_pred / nu1
K_sim <- 10000

set.seed(42)
simulated_sums <- replicate(K_sim, sum(rexp(100, rate = nu_pred)))

prob_safe <- mean(simulated_sums > 60)

cat("Using theoretical minimum nu =", round(nu_pred, 4), "\n")
cat("Simulated probability (S > 60):", round(prob_safe, 4), "\n")
cat("Desired probability: 0.95\n")
cat("Difference:", round(abs(prob_safe - 0.95), 4), "\n")

mean_S <- 100 / nu_pred
sd_S <- 10 / nu_pred

hist(simulated_sums, breaks = 50, probability = TRUE,
     main = "Distribution of Total Time until 100th Click",
     xlab = "Total time (seconds)", col = "lightblue", border = "white",
     xlim = c(55, 90)) 

abline(v = 60, col = "red", lwd = 2, lty = 2)

x_seq <- seq(min(simulated_sums), max(simulated_sums), length.out = 200)
lines(x_seq, dnorm(x_seq, mean = mean_S, sd = sd_S), 
      col = "darkgreen", lwd = 2)

legend("topright", 
       legend = c("Simulated data", "Safety threshold (60s)", "Normal approximation"),
       col = c("lightblue", "red", "darkgreen"), 
       lwd = c(10, 2, 2), lty = c(1, 2, 1))

cat("Maximum number of radioactive samples: N =", N_clt, "\n This ensures the location is safe with probability approx 0.95\n")
```

***Conclusion:*** 1. Central Limit Theorem (CLT) was successfully confirmed, showing that as the sample size ($n$) increased, the distribution of sample means rapidly converged to the Normal distribution. 2.The maximum number of radioactive samples ($N$) was determined by comparing theoretical bounds for the safety condition $\mathbb{P}(S > 60) \ge 0.95$. 3. The CLT provided the tightest and most accurate minimum required rate, $\nu \ge 1.3925$, significantly better than the Markov and Chernoff bounds. 4. Based on this result, the maximum allowed number of samples is determined to be $\mathbf{N=1}$ (since $\lceil 1.3925 / 44 \rceil = 1$). 5. Simulation verification using this critical rate confirmed the accuracy of the CLT, yielding a safety probability of $\approx 0.9501$.

**Next, proceed with all the remaining steps**

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

1.  **In this part, we discuss independence of random variables and its moments: expectation and variance.**

    1.  Suppose we have a random variable $X$. Explain why $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

    ```{r}
    cat("Question 1.1: Why E(1/X) is not equal to 1/E(X)?\n\n")


    cat("Expectation is a linear operator, but 1/X is a nonlinear function.\n")
    cat("Because of Jensen's inequality, for convex functions:\n")
    cat("E(f(X)) is greater or equal to f(E(X))\n\n")

    cat("Example: X takes values 1 and 3 with equal probability\n")
    cat("E(X) = (1+3)/2 = 2, so 1/E(X) = 1/2 = 0.5\n")
    cat("E(1/X) = (1/1 + 1/3)/2 = 0.667\n")
    cat("So 0.667 is not equal to 0.5\n\n")
    ```

    2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of $Y := \frac{1}{X}$ to calculate the values of $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$. Comment on the received results;

    ```{r}
    cat("Question 1.2: Simulation with Normal Distribution\n\n")

    team_id <- 34
    mu <- team_id
    sigma_squared <- 2 * team_id + 7
    sigma <- sqrt(sigma_squared)

    cat("Parameters: mu =", mu, ", sigma squared =", sigma_squared, "\n\n")

    set.seed(123)
    n <- 100
    X <- rnorm(n, mean = mu, sd = sigma)
    Y <- 1 / X

    mean_X <- mean(X)
    mean_Y <- mean(Y)
    one_over_mean_X <- 1 / mean_X

    cat("Sample mean of X:", round(mean_X, 4), "\n")
    cat("1 divided by mean of X:", round(one_over_mean_X, 4), "\n")
    cat("Sample mean of 1/X:", round(mean_Y, 4), "\n")
    cat("Difference:", round(mean_Y - one_over_mean_X, 4), "\n\n")

    cat("As expected, E(1/X) is not equal to 1/E(X).\n")
    cat("E(1/X) is larger because 1/x is a convex function.\n\n")
    ```

    3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?

    ```{r}
    cat("Question 1.3: QQ-plots and Scatterplots\n\n")

    set.seed(456)
    n <- 500
    lambda <- 2

    X_exp <- rexp(n, rate = lambda)
    Y_exp <- rexp(n, rate = lambda)
    Z_exp <- log(X_exp) + 5

    par(mfrow = c(2, 2))

    #1: scatterplot of X and Y (independent)
    plot(X_exp, Y_exp, pch = 16, col = rgb(0, 0, 1, 0.5),
     main = "Scatterplot: X vs Y (independent)",
     xlab = "X", ylab = "Y")
    grid()

    #2: QQ-plot of X and Y
    qqplot(X_exp, Y_exp, pch = 16, col = rgb(0, 0, 1, 0.5),
       main = "QQ-plot: X vs Y",
       xlab = "X quantiles", ylab = "Y quantiles")
    abline(0, 1, col = "red", lwd = 2)
    grid()

    #3: scatterplot of X and Z (dependent)
    plot(X_exp, Z_exp, pch = 16, col = rgb(1, 0, 0, 0.5),
     main = "Scatterplot: X vs Z (dependent)",
     xlab = "X", ylab = "Z = log(X) + 5")
    grid()

    # 4: QQ-plot of X and Z
    qqplot(X_exp, Z_exp, pch = 16, col = rgb(1, 0, 0, 0.5),
       main = "QQ-plot: X vs Z",
       xlab = "X quantiles", ylab = "Z quantiles")
    grid()

    par(mfrow = c(1, 1))

    cat("For X and Y (both exponential with lambda = 2):\n")
    cat("Scatterplot shows random cloud with no pattern, so they are independent\n")
    cat("QQ-plot shows points following diagonal line, so distributions are similar\n\n")

    cat("For X and Z (where Z = log(X) + 5):\n")
    cat("Scatterplot shows clear curved pattern, so they are dependent\n")
    cat("QQ-plot shows curved pattern, so distributions are different\n\n")

    cat("Summary:\n")
    cat("X and Y are independent and have similar distributions\n")
    cat("X and Z are dependent and have different distributions\n\n")

    ```

    ------------------------------------------------------------------------

2.  You toss a fair coin three times and a random variable $X$ records how many times the coin shows Heads. You convince your friend that they should play a game with the following payoff: every round (equivalent to three coin tosses) will cost £$1$. They will receive £$0.5$ for every coin showing Heads. What is the expected value and the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

    ```{r}
      # 2.1
      cat("Question 2.1: Type of Random Variable X\n\n")

      cat("X = number of Heads in 3 coin tosses\n\n")
      cat("X is binomially distributed\n\n")

      cat("We have n = 3 independent trials \n")
      cat("Each trial has two outcomes: Heads or Tails\n")
      cat("Probability of Heads is p = 0.5 \n")
      cat("X counts the number of Heads\n")
      cat("So X follows Binomial distribution with n=3 and p=0.5\n\n")
    ```

    2.  What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;

    ```{r}
    # 2.2
    cat("Question 2.2: Expected Value and Variance of X\n\n")
    n_trials <- 3
    p_heads <- 0.5

    #theoretical values
    E_X <- n_trials * p_heads
    Var_X <- n_trials * p_heads * (1 - p_heads)

    cat("Theoretical values:\n")
    cat("For X ~ Binomial(n=3, p=0.5):\n")
    cat("E(X) = n times p = 3 times 0.5 =", E_X, "\n")
    cat("Var(X) = n times p times (1-p) = 3 times 0.5 times 0.5 =", Var_X, "\n\n")

    set.seed(789)
    n_sim <- 100
    X_sim <- rbinom(n_sim, size = n_trials, prob = p_heads)

    sample_mean_X <- mean(X_sim)
    sample_var_X <- var(X_sim)

    cat("Simulated values (100 realizations):\n")
    cat("Sample mean:", round(sample_mean_X, 4), "\n")
    cat("Sample variance:", round(sample_var_X, 4), "\n\n")

    cat("Simulated values are close to theoretical values.\n")
    cat("Small differences are due to random sampling.\n")
    cat("With more simulations, results would be even closer.\n\n")

    barplot(table(X_sim) / n_sim, 
        main = "Distribution of X (Number of Heads in 3 tosses)",
        xlab = "Number of Heads", ylab = "Relative Frequency",
        col = "lightblue", ylim = c(0, 0.5))

    x_vals <- 0:3
    theo_probs <- dbinom(x_vals, size = n_trials, prob = p_heads)
    points(1:4 - 0.5, theo_probs, col = "red", pch = 19, cex = 1.5)
    legend("topright", legend = c("Simulated", "Theoretical"), 
       fill = c("lightblue", NA), pch = c(NA, 19), col = c(NA, "red"))
    ```

    3.  What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;

    ```{r}
       # 2.3
    cat("\nQuestion 2.3: Expected Value and Variance of Y\n\n")

    cat("Y = 0.5 times X minus 1 \n\n")

    #theoretical values
    E_Y <- 0.5 * E_X - 1
    Var_Y <- (0.5^2) * Var_X

    cat("Theoretical values:\n")
    cat("Using properties of expectation and variance:\n")
    cat("E(Y) = 0.5 times E(X) minus 1 = 0.5 times 1.5 minus 1 =", E_Y, "\n")
    cat("Var(Y) = 0.5 squared times Var(X) = 0.25 times", Var_X, "=", Var_Y, "\n\n")

    Y_sim <- 0.5 * X_sim - 1

    sample_mean_Y <- mean(Y_sim)
    sample_var_Y <- var(Y_sim)

    cat("Simulated values (100 realizations):\n")
    cat("Sample mean:", round(sample_mean_Y, 4), "\n")
    cat("Sample variance:", round(sample_var_Y, 4), "\n\n")

    cat("Simulated values match theoretical values well.\n")
    cat("The expected payoff E(Y) = -0.25 means the player loses on average.\n")
    cat("This is not a fair game!\n\n")

    cat("Each round costs 1 pound\n")
    cat("Expected number of Heads: 1.5\n")
    cat("Expected payout: 0.5 times 1.5 = 0.75 pounds\n")
    cat("Net expected gain: 0.75 minus 1 = -0.25 pounds (a loss!)\n")
    cat("Your friend is being tricked!\n\n")

    hist(Y_sim, breaks = seq(-1.5, 0.5, by = 0.5), probability = TRUE,
     main = "Distribution of Y (Payoff from Game)",
     xlab = "Payoff (pounds)", ylab = "Relative Frequency",
     col = "lightcoral", border = "white")
    abline(v = E_Y, col = "blue", lwd = 3, lty = 2)
    text(E_Y, par("usr")[4]*0.9, paste("E(Y) =", E_Y), pos = 4, col = "blue")
    ```

    ##Concluion for 4

We explained theoretically and showed with an example why E(1/X) ≠ 1/E(X) — due to nonlinearity and Jensen’s inequality.

We simulated a normal case and observed that the mean of the reciprocals differs from the reciprocal of the mean (for μ = 34, the difference was small but consistent).

For two independent exponential samples (X, Y), the scatterplot looks like random noise, and the QQ-plot shows that the distributions are similar. In contrast, the pair (X, Z), where Z = log(X) + 5, shows clear dependence and different distributions.

The binomial example (3 coin tosses) showed that X \~ Bin(3, 0.5) with E(X) = 1.5 and Var(X) = 0.75. For the payoff Y = 0.5 X - 1, we get E(Y) = -0.25 and Var(Y) = 0.1875, which means the game is disadvantageous for the player.

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### General summary and conclusions

Summarize here what you've done, whether you solved the tasks, what difficulties you had etc.
